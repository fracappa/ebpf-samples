package main

import (
	"fmt"
	"log"
	"net"
	"os"
	"time"

	"github.com/cilium/ebpf/rlimit"
	"github.com/florianl/go-tc"
	"github.com/florianl/go-tc/core"
	"golang.org/x/sys/unix"
)

//go:generate go run github.com/cilium/ebpf/cmd/bpf2go dropicmp tc.c

const vethIface = "veth1"

/*
This program demonstrates how to load and attach a TC eBPF program to an already
created interface using the cilium/ebpf and florianl/go-tc pkgs.

As you may notice, in the go:generate there is a dropicmp, this means that the
autogenerated files and many autogenerated structured will have this dropicmp prefix.

Basically for the tc part i'm using this example
https://github.com/florianl/go-tc/blob/1f6cf4701feb4f6aaaf1fb50f11676b8090cc9ec/example_gteq_1.16_test.go#L22
thanks to florianl. In this example he is creating a dummy interface and he is attaching a tc ebpf program that
is return 0 (TC_ACT_OK).

The ebpf code is this example2 is the same as example1.

TO test it I chose to create a simple testbed where we have (default ns)[veth1]---[veth2](ns2)
We will put the TC ebpf program on the ingress of veth1 that is in the default ns
and then ping from ns2 to see that all icmp echo request packets are dropped or also we can ping
from the default ns to see that all the icmp echo reply packets are dropped.

These are the commands to test it:

sudo ip link add name veth1 type veth peer name veth2
sudo ip netns add ns2
sudo ip link set veth2 netns ns2
sudo ip netns exec ns2 ip link set dev veth2 up
sudo ip link set veth1 up
sudo ip netns exec ns2 ip addr add 10.0.0.2/24 dev veth2
sudo ip addr add 10.0.0.1/24 dev veth1

# launch the ping command
sudo ip netns exec ns2 ping 10.0.0.1

# in another terminal
cd ebpf-examples
sudo ./tc/example2/bin/example2

We can see that that are no more echo reply packets.

# Now we can stop the program and then remove the clasct and we can see the echo reply packets

sudo tc qdisc del dev veth1 clsact

We can now delete the testbed (the veth1 peer in the default ns is automatically deleted)

sudo ip netns del ns2
*/
func main() {
	// Allow the current process to lock memory for eBPF resources.
	if err := rlimit.RemoveMemlock(); err != nil {
		log.Fatal(err)
	}

	// Load pre-compiled programs and maps into the kernel.
	objs := dropicmpObjects{}
	if err := loadDropicmpObjects(&objs, nil); err != nil {
		log.Fatalf("loading drop icmp objects: %v", err)
	}
	defer objs.Close()

	iface, err := net.InterfaceByName(vethIface)
	if err != nil {
		fmt.Fprintf(os.Stderr, "could not get interface ID: %v\n", err)
		return
	}

	// Open a netlink/tc connection to the Linux kernel. This connection is
	// used to manage the tc/qdisc and tc/filter to which
	// the eBPF program will be attached
	tcnl, err := tc.Open(&tc.Config{})
	if err != nil {
		fmt.Fprintf(os.Stderr, "could not open rtnetlink socket: %v\n", err)
		return
	}
	defer func() {
		if err := tcnl.Close(); err != nil {
			fmt.Fprintf(os.Stderr, "could not close rtnetlink socket: %v\n", err)
		}
	}()

	// Create a qdisc/clsact object that will be attached to the ingress part
	// of the networking interface.
	qdisc := tc.Object{
		Msg: tc.Msg{
			Family:  unix.AF_UNSPEC,
			Ifindex: uint32(iface.Index),
			Handle:  core.BuildHandle(tc.HandleRoot, 0x0000),
			Parent:  tc.HandleIngress,
			Info:    0,
		},
		Attribute: tc.Attribute{
			Kind: "clsact",
		},
	}

	// Attach the qdisc/clsact to the networking interface.
	if err := tcnl.Qdisc().Add(&qdisc); err != nil {
		fmt.Fprintf(os.Stderr, "could not assign clsact to %s: %v\n", iface.Name, err)
		return
	}
	// When deleting the qdisc, the applied filter will also be gone
	defer tcnl.Qdisc().Delete(&qdisc)

	fd := uint32(objs.TcIngressF.FD())
	flags := uint32(0x1)

	// Create a tc/filter object that will attach the eBPF program to the qdisc/clsact.
	filter := tc.Object{
		Msg: tc.Msg{
			Family:  unix.AF_UNSPEC,
			Ifindex: uint32(iface.Index), // iface.Index of the veth1 itnerface in the default namespace
			Handle:  0,
			Parent:  core.BuildHandle(tc.HandleRoot, tc.HandleMinIngress),
			Info:    0x300,
		},
		Attribute: tc.Attribute{
			Kind: "bpf",
			BPF: &tc.Bpf{
				FD:    &fd,
				Flags: &flags,
			},
		},
	}

	// Attach the tc/filter object with the eBPF program to the qdisc/clsact.
	if err := tcnl.Filter().Add(&filter); err != nil {
		fmt.Fprintf(os.Stderr, "could not attach filter for eBPF program: %v\n", err)
		return
	}

	// Periodically read the value from the counter map and log it.
	ticker := time.NewTicker(1 * time.Second)
	defer ticker.Stop()
	for range ticker.C {
		var value uint32
		if err := objs.DroppedMap.Lookup(uint32(0), &value); err != nil {
			log.Fatalf("reading dropped map: %v", err)
		}
		log.Printf("Counter %d\n", value)
	}

}
